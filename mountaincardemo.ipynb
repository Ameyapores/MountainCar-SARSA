{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretized SARSA for mountain-car problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Class Mountain car: \n",
    "\n",
    "Defines the environment consisting of a hill .Then provide bounds for the maximum and minimum velocity and position. Next, define the action space. \n",
    "\n",
    "Once the environment is setup, we will discretise the continuous state and compute the value for each state-action pair possible. Further, actions are taken according to the epsilon-greedy algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCar(object):\n",
    "    def __init__(self,alpha=0.3,gamma=1.0,epsilon=0.01):\n",
    "        super(MountainCar, self).__init__()          \n",
    "        self.alpha       = alpha    #learning rate\n",
    "        self.gamma       = gamma    #discount factor\n",
    "        self.epsilon     = epsilon  #probability of a random action selection\n",
    "        self.statelist   = self.BuildSateList()     # the list of states\n",
    "        self.actionlist  = self.BuildActionList()   # the list of actions        \n",
    "        self.nactions    = self.actionlist.shape[0] # number of actions\n",
    "        self.Q           = self.BuildQTable()       # the Qtable\n",
    "\n",
    "    def BuildSateList(self):\n",
    "        # state discretization for the mountain car problem\n",
    "        xdiv  = (0.55-(-1.5))   / 10.0\n",
    "        xpdiv = (0.07-(-0.07)) / 5.0\n",
    "        \n",
    "        x = np.arange(-1.5,0.5+xdiv,xdiv)\n",
    "        xp= np.arange(-0.07,0.07+xpdiv,xpdiv)\n",
    "\n",
    "        N=x.size\n",
    "        M=xp.size\n",
    "\n",
    "        states=[] #zeros((N*M,2)).astype(Float32)\n",
    "        #index=0\n",
    "        for i in range(N):    \n",
    "            for j in range(M):\n",
    "                states.append([x[i], xp[j]])        \n",
    "        return np.array(states)\n",
    "\n",
    "    def BuildActionList(self):\n",
    "        return np.array([-1.0 , 0.0 , 1.0])\n",
    "\n",
    "    def BuildQTable(self):\n",
    "        nstates     = self.statelist.shape[0]\n",
    "        nactions    = self.actionlist.shape[0]\n",
    "        Q = [[0.0 for i in range(nactions)] for i in range(nstates)]\n",
    "        return Q\n",
    "\n",
    "    def GetReward(self, x ):\n",
    "        # MountainCarGetReward returns the reward at the current state\n",
    "        # x: a vector of position and velocity of the car\n",
    "        # r: the returned reward.\n",
    "        # f: true if the car reached the goal, otherwise f is false\n",
    "            \n",
    "        position = x[0]\n",
    "        # bound for position; the goal is to reach position = 0.45\n",
    "        bpright  = 0.45\n",
    "\n",
    "        r = -1\n",
    "        f = False\n",
    "        \n",
    "        \n",
    "        if  position >= bpright:\n",
    "            r = 100\n",
    "            f = True\n",
    "        \n",
    "        return r,f\n",
    "\n",
    "    \n",
    "    def DoAction(self, force, x ):\n",
    "        #MountainCarDoAction: executes the action (a) into the mountain car\n",
    "        # a: is the force to be applied to the car\n",
    "        # x: is the vector containning the position and speed of the car\n",
    "        # xp: is the vector containing the new position and velocity of the car\n",
    "\n",
    "        position = x[0]\n",
    "        speed    = x[1] \n",
    "\n",
    "        # bounds for position\n",
    "        bpleft=-1.5 \n",
    "\n",
    "        # bounds for speed\n",
    "        bsleft=-0.07 \n",
    "        bsright=0.07\n",
    "         \n",
    "        speedt1= speed + (0.001*force) + (-0.0025 * math.cos( 3.0*position) )\t \n",
    "        speedt1= speedt1 * 0.999 # thermodynamic law, for a more real system with friction.\n",
    "\n",
    "        if speedt1<bsleft: \n",
    "            speedt1=bsleft \n",
    "        elif speedt1>bsright:\n",
    "            speedt1=bsright    \n",
    "\n",
    "        post1 = position + speedt1 \n",
    "\n",
    "        if post1<=bpleft:\n",
    "            post1=bpleft\n",
    "            speedt1=0.0\n",
    "            \n",
    "        xp = np.array([post1,speedt1])\n",
    "        return xp\n",
    "\n",
    "\n",
    "    def GetInitialState(self):\n",
    "        initial_position = -0.5\n",
    "        initial_speed    =  0.0    \n",
    "        return  np.array([initial_position,initial_speed])\n",
    "\n",
    "    def DiscretizeState(self,x):    \n",
    "        \"\"\"DiscretizeState check which entry in the state list is more close to x and return the index of that entry.\"\"\"\n",
    "        \n",
    "        edist2   = np.sqrt(np.sum((self.statelist-x)**2,1))\n",
    "        return     np.argmin(edist2) \n",
    "\n",
    "    def e_greedy_selection(self, s):\n",
    "        #selects an action using Epsilon-greedy strategy\n",
    "        # Q: the Qtable\n",
    "        # s: the current state        \n",
    "                \n",
    "        if (random.random()>self.epsilon):\n",
    "            a = self.GetBestAction(s)    \n",
    "        else:\n",
    "            # selects a random action based on a uniform distribution\n",
    "            a = random.randint(0, 2)\n",
    "            #print(a)\n",
    "        return a\n",
    "        \n",
    "    def GetBestAction(self, s ):\n",
    "        #GetBestAction return the best action for state (s)\n",
    "        #Q: the Qtable\n",
    "        #the current state\n",
    "        #has structure  Q(states,actions)\n",
    "        \n",
    "        #a = argmax(self.Q[s,:].flat)\n",
    "        a  = np.argmax(self.Q[s])    \n",
    "        return a\n",
    "\n",
    "    def UpdateSARSA(self, s, a, r, sp, ap):\n",
    "        \"\"\" Update Qtable using SARSA\"\"\"\n",
    "        #print (s,sp, self.Q)\n",
    "        self.Q[s][a]  = self.Q[s][a] + self.alpha * ( r + self.gamma*self.Q[sp][ap] - self.Q[s][a] )\n",
    "\n",
    "    def SARSAEpisode(self, maxsteps=100, grafic = False):\n",
    "        # do one episode with sarsa learning\n",
    "        # maxstepts: the maximum number of steps per episode\n",
    "        # Q: the current QTable\n",
    "        # alpha: the current learning rate\n",
    "        # gamma: the current discount factor\n",
    "        # epsilon: probablity of a random action\n",
    "        # statelist: the list of states\n",
    "        # actionlist: the list of actions\n",
    "        \n",
    "        x                = self.GetInitialState()\n",
    "        steps            = 0\n",
    "        total_reward     = 0\n",
    "\n",
    "\n",
    "        # convert the continous state variables to an index of the statelist\n",
    "        s   = self.DiscretizeState(x)\n",
    "        \n",
    "        # selects an action using the epsilon greedy selection strategy\n",
    "        a   = self.e_greedy_selection(s)\n",
    "\n",
    "\n",
    "        for i in range(1,maxsteps+1):\n",
    "                        \n",
    "            # convert the index of the action into an action value\n",
    "            action = self.actionlist[a]    \n",
    "            \n",
    "            # do the selected action and get the next car state    \n",
    "            xp     = self.DoAction( action , x )    \n",
    "            \n",
    "            # observe the reward at state xp and the final state flag\n",
    "            r,isfinal    = self.GetReward(xp)\n",
    "            total_reward = total_reward + r\n",
    "            \n",
    "            # convert the continous state variables in [xp] to an index of the statelist\n",
    "            sp     = self.DiscretizeState(xp)\n",
    "            \n",
    "            # select action prime\n",
    "            ap     = self.e_greedy_selection(sp)\n",
    "            #print(ap)\n",
    "            \n",
    "            # Update the Qtable, that is,  learn from the experience\n",
    "            self.UpdateSARSA( s, a, r, sp, ap)\n",
    "            \n",
    "            \n",
    "            #update the current variables\n",
    "            s = sp\n",
    "            a = ap\n",
    "            x = xp\n",
    "            \n",
    "            #if graphic function then show graphics\n",
    "            if grafic:\n",
    "                self.PlotFunc(x,a,steps)\n",
    "                \n",
    "            \n",
    "            #increment the step counter.\n",
    "            steps = steps+1\n",
    "            \n",
    "            # if reachs the goal breaks the episode\n",
    "            if isfinal==True:\n",
    "                break\n",
    "        \n",
    "        return total_reward,steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-3: \n",
    "Run the Code for 200 episodes. Each episode has maximum steps of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MountainCarDemo(maxepisodes):\n",
    "    MC  = MountainCar()\n",
    "    maxsteps = 1000\n",
    "    grafica  = False\n",
    "    \n",
    "    xpoints=[]\n",
    "    ypoints=[]\n",
    "    \n",
    "    for i in range(1, maxepisodes+1):    \n",
    "    \n",
    "        total_reward,steps  = MC.SARSAEpisode( maxsteps, grafica )    \n",
    "\n",
    "        \n",
    "        print ('Espisode: ',i,'  Steps:',steps,'  Reward:',str(total_reward),' epsilon: ',str(MC.epsilon))\n",
    "        \n",
    "        MC.epsilon = MC.epsilon * 0.99\n",
    "        \n",
    "        xpoints.append(i)\n",
    "        ypoints.append(-total_reward)\n",
    "    plt.plot(xpoints, ypoints)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"-Rewards\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    MountainCarDemo(200) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
